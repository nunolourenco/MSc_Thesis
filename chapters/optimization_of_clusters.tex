%!TEX root = /Users/nunolourenco/Documents/FCTUC/Mestrado/2010_2011/Thesis/Thesis/thesis.tex
\chapter{Optimization Algorithms}
	\label{chap:opt_alg}
		
	    In this chapter we describe some approximation methods for global optimization. We do not aim to provide a comprehensive presentation of optimization algorithms. We offer just a brief description of several methods that focus on global, unbiased, iterative, approximation algorithms (a class usually known as \emph{metaheuristics}). The reason for this choice is that current state-of-art methods for cluster geometry optimization belong to this class. In Section \ref{subsec:single_state_solution} we briefly highlight single-solution methods, followed by Evolutionary Algorithms in Section \ref{subsec:evolutionary_algorithms}. In Section \ref{subsec:swarm_intelligence}, we describe example of swarm intelligence algorithms, focused on ant colony optimization methods. Finally, in Section \ref{subsec:cgo}, we give a brief description of how some of the presented methods were applied to the problem of cluster geometry optimization. 
		\section{Single-solution Methods}
			\label{subsec:single_state_solution}
			%SA
		 	Single-solution algorithms generate one initial solution and try to improve it iteratively. A remarkable example is Simulated Annealing in which has been applied to the cluster geometry optimization problem. However, it is not the only single-solution method. Hill-Climbing (HC), Tabu Search (TS), Iterated Local Search (ILS) or Variable Neighborhood Search (VNS) are also other examples of single-solution methods. For details on these methods, the reader can see, e.g., \cite{luke09}.  		
			
			\subsubsection*{Simulated Annealing}
			Simulated Annealing (SA) \cite{kirkpatrick83} is a generic probabilistic metaheuristic. The inspiration to this method comes from annealing in metallurgy, a technique involving heating and controlled cooling of a material. The heat causes the atoms constituting the material to become unstuck from their initial positions (a local minimum) and wander randomly through space. Then, the slow cooling gives them more chances of gradually finding configurations with a lower internal energy than the initial one.

			Analogously with this physical process, each step of the SA selects a nearby solution. If the new solution is better than the current one, the new solution replaces the current. However, if the new solution is worse than the current it can still be selected. Acceptance depends of two criteria:
			\begin{enumerate}
				\item How worse is the new solution.
				\item How many steps have been performed.
			\end{enumerate}
			The number of steps of the algorithm is controlled by $T$, which is called temperature. It starts with an initial high value, that decreases over time, until it reaches the minimum value of $T_{min}$. The generic behavior is shown in Algorithm \ref{alg:sa}.
			
			
			\begin{algorithm}
				\caption{Simulated Annealing}
				\label{alg:sa}
				\begin{algorithmic}
				\STATE define a random initial solution $s$
				\STATE $best\_solution \gets s$ 
				\WHILE{termination condition not met}
					\STATE choose a neighbor solution $snew$
					\IF{accept($s$,$snew$,$T$)} 
						\STATE  $s \gets snew$
					\ENDIF
					\IF{is\_better(best\_solution,s)} 
						\STATE  $best\_solution \gets s$
					\ENDIF
					\STATE decrease($T$)
				\ENDWHILE
				\STATE return best\_solution
				\end{algorithmic}
			\end{algorithm}
		
		
		\noindent The \emph{accept()} procedure checks if the new solution is to be accepted. \emph{is\_better()} check if the current solution is better than the best solution found until that moment.

			
		\section{Evolutionary Algorithms}
			\label{subsec:evolutionary_algorithms}
			
			Evolutionary Algorithms (EA) are a family of algorithms whose main inspiration is the biological evolution \cite{eiben03}. EAs use the reproduction, mutation, recombination and selection mechanisms, which play an important role in biological evolution.	
			EAs maintain a population of individual solutions, which can be selected for reproduction. Selection is probabilistic and biased by the fitness of the individuals. In reproduction, two selected individuals exchange information about their solutions. This process is called crossover and it creates two new solutions. Since the parents are promising solutions, with crossover we aim to create two new solutions that, hopefully, are promising as well. 
			Next, mutation slightly modifies the offspring. The application of the mutation operator is probabilistic and it aims to maintain diversity in the population.
			In the last step, offsprings replace the old population, and a new iteration starts.
			The general algorithm is presented in Algorithm \ref{alg:genetic_algorithm}.
			\begin{algorithm}
				\caption{General Evolutionary Algorithm}
				\label{alg:genetic_algorithm}
				\begin{algorithmic}
				\STATE randomly generate initial population
				\STATE evaluate initial population
				\WHILE{termination condition not met}
					\STATE select parents
					\STATE crossover
					\STATE mutation
					\STATE evaluate offspring
					\STATE replacement
				\ENDWHILE
				\STATE return best individual in the population
				\end{algorithmic}
			\end{algorithm} 
			
			For more details on evolutionary algorithms see, e.g., \cite{eiben03}.
			
		\section{Swarm Intelligence}
		\label{subsec:swarm_intelligence}
		The idea of swarm intelligence is to develop algorithms that model the behavior of a group of animals that engage in social interactions. Some examples are synchronous bird flocking, ants gathering food or fish shoals escaping from a predator \cite{si01}. In the following sections we describe the Ant Colony Optimization (ACO) framework, which is inspired by the behavior of foraging ants searching for food.
		
		\subsubsection*{Traveling Salesman Problem}
		 The Traveling Salesman Problem (TSP) will be used to help explain the ACO algorithms described. We choose this problem, due to its relevance, and because it is a benchmark for all ACO algorithms.
		
		The TSP is the problem of a salesman, who starting from a initial city, wants to find the shortest tour that visits a set of cities. More formally, we have a fully connected, undirected graph $G=(C,E)$, where $C$ contains the cities that the salesman has to visit, and $E$ are the arcs connecting the cities. The problem consists of finding the minimum length Hamiltonian circuit of the graph, where an Hamiltonian circuit is a closed path visiting each node $c \in C$ exactly once.
		%ACO
		\subsubsection*{Ant Colony Optimization Algorithms}
		\label{subsubsec:ant_algorithms}

		Ant colonies are distributed systems \cite{acobook} composed by a large amount of simple agents. They maintain a highly structured social organization, which allows the colonies to accomplish complex tasks. The individuals in the colony communicate with each other, via \emph{stigmergy}, which is a form of indirect communication mediated by modifications in the environment where the ants are working. In concrete, they have a chemical called pheromone that they deposit in the environment, which will be sensed by other ants, and help them in their work. One example of pheromones is the \emph{``trail pheromone"}, that is very important for the social life of some ant species. This specific type of pheromone allows ants to mark paths, for example, from the nest to food source.
	
		The idea behind ant algorithms is then to use a form of \emph{``artificial stigmergy"}, to coordinates artificial ant colonies. With this in mind, Marco Dorigo proposed an ACO algorithm in 1992 \cite{dorigo92}. 	

		Algorithm \ref{alg:general_aco} presents the general idea behind the ACO algorithm.
				\begin{algorithm}
					\caption{General ACO Algorithm}
					\label{alg:general_aco}
					\begin{algorithmic}
					\WHILE{termination condition not met}
						\STATE ConstructSolutions()
						\STATE UpdatePheromones() 
						\STATE DaemonActions() \COMMENT {optional}
					\ENDWHILE
					\end{algorithmic}
				\end{algorithm}
				
				In ACO a problem is represented as a graph $G$ that artificial ants will cross when building solutions. $G$ is composed composed by a set of solution components $C$, connected by a set $E$ of edges. The specific composition of $G$ depends on the problem being solved.
				 Using the TSP as an example, the cities that the salesman has to visit correspond to $C$ and the routes that he has to travel correspond to $E$.			
				In the \emph{ConstructSolutions()} procedure, each ant navigates the graph $G$, and constructs a solution. This solution is built incrementally by stochastic local decisions that make use of pheromone trails and heuristic information about the problem. Once an ant has built a solution, or while the solution is being built, it evaluates the (partial) solution.
				
				The \emph{UpdatePheromones()} is the process by which the pheromone trails are updated. The trails values can either increase, as ants deposits pheromone in the components or edges that they use, or decrease, due to pheromone evaporation. In practice, the deposit of pheromone trails increases the probability that those components/edges, which belonged to good solutions, will be selected again. Differently, pheromone evaporation implements a forgetting method: it avoids early convergence to a suboptimal region, therefore favoring exploration of new areas of the search space.
				 
				Finally \emph{DaemonActions()} procedure corresponds to actions that are not performed by ants. Examples of this daemon actions are the local search procedures, or the collection of global information to be used whether it is useful or not to deposit additional pheromone to bias the search process from a non local perspective.
				
				During the years several ACO algorithms were proposed. Next we introduce and describe the some of these proposed algorithms. 

					\subsubsection*{Ant System} 
					\label{sec:ant_system}
					Ant System (AS) was first proposed in 1992 by Marco Dorigo \cite{dorigo92, dorigo96}.
					Initially, the pheromone trails are initialized according to a simple rule: the initial value must be higher than the total amount of pheromone that an ant will deposit in one iteration. Heuristics to calculate this value are commonly used.

					Each ant in AS constructs a complete solution to the problem in question. For instance, in the TSP, an ant must construct a tour. At each construction step, an ant uses a probabilistic rule to decide which new component it should add to the current solution. Following \cite{dorigo96}, the probability with which ant $k$, currently at city $i$, chooses to go to city $j$ is given as follows:
					\begin{equation}
						\label{eq:prob_rule_for_choice}
						p_{ij}^k = \frac{[\tau_{ij}]^\alpha[\eta_{ij}]^\gamma} {\sum_{l \in N_{i}^k} [\tau_{il}]^\alpha[\eta_{il}]^\gamma}, \quad j \in N_{i}^k,
					\end{equation}

		\noindent where $\tau_{ij}$ is the pheromone trail in edge $(i,j)$, $\eta$ is an heuristic value that is problem related (e.g. for the TSP: $\eta = \frac{1}{d_{ij}}$, where $d_{ij}$ is the distance between city $i$ and city $j$), $N_i^k$ is the feasible neighborhood when the ant $k$ is in the node $i$ (e.g. in TSP is the set of cities that ant $k$ has not visited yet), $\alpha, \gamma$ are parameters of the algorithm. The $\alpha$ parameter determines the relative influence of the pheromone trail, and $\gamma$ determines the relative influence of the heuristic information.

					After all ants have constructed their solutions, the pheromone trails must be updated. First, and if we consider pheromone evaporation \cite{dorigo96}, we must update the trail by taking into account the percentage of pheromone to evaporate:
					\begin{equation}
						\tau_{ij} = (1 - \rho)\tau_{ij}, \quad \forall(i,j)\in E,
					\end{equation}
		\noindent where $0 \leq \rho \leq 1$ is a parameter that determines the quantity of pheromones to evaporate.

					Then we perform the  pheromone trails update \cite{dorigo96}:
					\begin{equation}
						\label{eq:as_pheromone_update}
						\tau_{ij} = \tau_{ij} + \sum_{k=1}^{m}\Delta\tau_{ij}^k, \quad \forall(i,j)\in E,
					\end{equation}


		\noindent where $m$ is the total number of ants, $\Delta\tau^k$ is calculated using $\frac{1} {L^k}$, where $L^k$ is the fitness of the solution of the $k^{th}$-ant.

					This algorithm had a poor performance when applied to larger instances of optimization problems \cite{acobook}, like the TSP. Therefore, there have been attempts to improve the algorithm. Elitist Ant System(EAS) \cite{dorigo92, dorigo96}, Rank-Based Ant System($AS_{rank}$) \cite{bullnheimer97}, Ant Colony System \cite{dorigo97}, and Max-Min Ant System (MMAS) \cite{stutzle00} are the most relevant contributions.

					\subsubsection*{Elitist Ant System}

					The main idea behind Elitist Ant System (EAS) \cite{dorigo92, dorigo96} is that the edges belonging to the best solution found since the beginning of the algorithm should receive more pheromones. Adding an additional parameter that reinforces pheromones if an edge belongs to the best tour so far to Eq. \eqref{eq:as_pheromone_update} \cite{dorigo92, dorigo96}, the final equation will be:
					\begin{equation}
						\label{eq:eas_pheromone_update}
						\tau_{ij} = \tau_{ij} + \sum_{k=1}^{m}\Delta\tau_{ij}^k + e\Delta\tau^{bs},
					\end{equation}

		\noindent where \emph{e} is the weight given to the best tour so far, and $\Delta\tau^{bs}$ is determined using $\frac{1} {L^{bs}}$, where ${L^bs}$ is the fitness of the best solution so far. 

					\subsubsection*{Rank-Based Ant System}

					In $AS_{rank}$ each ant deposits pheromones according to its rank, when compared to other ants of the colony. Besides the best-so-far-ant deposits an additional amount of pheromone.

					Before the updating of pheromones, ants are sorted, by ascending order, of the quality of the current solution. In each iteration, only the $(w-1)$ best ranked ants, and the best ant so far, can deposit pheromone, where $w$ is a parameter. Thus, the $AS_{rank}$ pheromone update rule is as follow \cite{bullnheimer97}:
					\begin{equation}
						\label{eq:asrank_pheromone_update}
						\tau_{ij} = \tau_{ij} + \sum_{r=1}^{w-1}(w-r)\Delta\tau_{ij}^r + w\Delta\tau_{ij}^{bs},
					\end{equation}.

					\subsubsection*{Ant Colony System}
					\label{sec:acs}
					This algorithm \cite{dorigo97}, despite of being inspired by the AS, differs from it in three main aspects:
					\begin{enumerate}
						\item It exploits the accumulated search experience, through the use of a more aggressive action choice rule: Eq. \ref{eq:acs_choice_rule}. 
						\item Pheromone update and pheromone deposit takes place only on the edges belonging to the \emph{best-so-far} solution;
						\item It uses an online pheromone update strategy. Each time an ant, while building solutions, uses an edge $(i,j)$, it removes some pheromone from it. This increase the exploration of alternative solutions.
					\end{enumerate}

					In Ant Colony System(ACS), when an ant is building a solution, it chooses to move from component $i$ to component $j$ according to the following \emph{pseudorandom proportional} rule \cite{dorigo97}:

					\begin{equation} 
						\label{eq:acs_choice_rule}
						j =
						  \begin{cases}
						   \operatorname{arg\,\max}_{l \in N_{i}^k} (\tau_{il}[\eta_{il}^{\gamma}]), & \text{if } q \leq q_{0}  \\
						  	P, & \text{otherwise,}
						  \end{cases}
					\end{equation}

					\noindent where $q$ is a uniformly distributed variable in the range $[0,1]$, $q_{0}, 0 \leq q_{0} \leq 1$, is a parameter, $P$ is a random variable selected according to Eq. \eqref{eq:prob_rule_for_choice}, and $\gamma$ is a parameter. The $q_{0}$ parameter defines the degree of exploration and the choice of whether to concentrate the search around the \emph{best-so-far}, or explore other locations.


					\subsubsection*{$\mathcal{MAX}-\mathcal{MIN}$ Ant System}
					$\mathcal{MAX}-\mathcal{MIN}$ Ant System ($\mathcal{MM}$AS) introduces four main modifications to the traditional AS \cite{stutzle00}:
					\begin{enumerate}
						\item only the \emph{best-so-far} or the \emph{iteration-best} ants are allowed to deposit pheromones:
							\begin{equation}
								\label{eq:max_min_rank_pheromone_update}
								\tau_{ij} = \tau_{ij} + \Delta\tau_{ij}^{best}
							\end{equation}
							\noindent where $\Delta\tau^{best}$ is the computed using $\frac{1} {L^{best}}$, ${L^{best}}$ is the fitness of the best solution so far or the fitness of the best solution in the iteration, depending of which one is used. In most cases \emph{best-so-far} and \emph{iteration-best} according to a schedule, that can be based, for example, in the number of iterations.
						
						\item Introduction of a limit to pheromone trails $[\tau_{min},\tau_{max}]$. This modification intends to counteract the effect of stagnation, that is the situation where all ants build the same solution.
						
						\item In the beginning, all the pheromone trails are initialized with $\tau_{max}$. Thus, and with small pheromone evaporation rate, the exploration is increased in the early stages of the search.
						
						\item Pheromone trails are reinitialized every time the system approaches stagnation, or when no improved tours have been generated for a certain number of consecutive generations.
						 					
					\end{enumerate}
		
					

					\subsubsection*{Multi-colony ant algorithm}
					The main idea behind this algorithm is to have a set of identical colonies solving the same problem. In \cite{melo10} they used a variant of ACS with multiple colonies working in different locations of the search space, where the best colonies share information with the worst. This information sharing is made by passing along, to the worst colony, the \emph{best-so-far} solution. Every time a migration occurs, the worst colony uses the new solution as if it was found by it.


					\subsubsection*{Approximate Nondeterministic Tree Search}
					Approximate nondeterministic tree search (ANTS) is an ACO algorithm which exploits the ideas from mathematical programming \cite{Maniezzo99}. ANTS computes lower bounds every time it adds a new component to a partial solution, and then uses it to define the heuristic information necessary to build solutions. The use of this lower bounds has advantages, since we can discard a component if it leads to partial solutions that have a larger cost than the best-so-far solution. However calculating lower bounds at each step, can introduce a higher computational overhead.


					\subsubsection*{Hyper-Cube Framework for Ant Colony Optimization}
					The hyper-cube framework was introduced in \cite{blum04} to automatically rescale the values of pheromone trails, so they can stay in range $[0,1]$. This choice was inspired by other algorithms that used binary representation of the solutions. In this type of representation, the decision variables typically correspond to solution components as they are used by the ants, that is, if a component is in the ant solution, it takes the value 1, else it takes the value 0. Thus, a solution corresponds to one corner of a \emph{n-dimensional} cube, where n is the number of decision variables.
					The relationship with ACO lies in the normalization of the pheromones to the interval [0,1]. In this case, the pheromone vector is a point in the solution space; in case $\tau$ is a binary vector, which corresponds to a solution of the problem.
					
					\subsubsection*{Ant Colony Optimization for continuous domains}
					ACO algorithms were originally proposed to solve discrete problems. Yet there are problems that are not discrete. Thus, and trying to keep the biological inspiration, there are some proposals to solve continuous problems using an ACO framework. 
					Here we present one of the main approaches, proposed by Socha et al.\cite{socha04}. For other alternatives in ACO for continuous problems see \cite{bilchev95, kong06, tsutsui04}.
					
					
					\subsubsection*{Ant Colony Optimization for continuous and Mixed-Variable Optimization}
					 ACO for continuous and Mixed-Variable Optimization($ACO_\mathbb{R}$) uses a simple approach: if a discrete probability distribution is used for discrete problems, then a continuous probability distribution function - the \emph{Probability Density Function} (PDF), should be adopted for continuous problems \cite{socha04}.

					The most popular PDF is the normal (or gaussian) function, defined by,
						\begin{equation}
							f(x) = \frac{1}{{\sigma \sqrt {2\pi } }}e^{{{ - \left( {x - \mu } \right)^2 } \mathord{\left/ 				{\vphantom {{ - \left( {x - \mu } \right)^2 } {2\sigma ^2 }}} \right. \kern-\nulldelimiterspace} {2\sigma ^2 }}}
						\end{equation}

					\noindent where $\mu$ is the mean and the $\sigma$ is the variance. The use of the normal distribution has one drawback, because it can not describe a situation where there are two promising disjoint locations in the search space, as it only has one maximum. To solve this problem, the algorithm uses a mixture of normal distributions, as we can se in Fig. \ref{fig:kernel_mixture1}. 
					
						\botapic[0.7]{kernel_mixture1}{Example of a mixture of gaussian kernels - The dashed line is mixture of all the other depicted gaussian functions}
						
						While building solutions, an ant, at a certain step $i$, generates a random number according to an PDF that composes the kernel mixture, and add it to the solution. This process is repeated until a complete solution has been constructed. Then solutions are evaluated.
						
						After the evaluation process we have to update the pheromone values. Hence we have to reinforce the influence of functions that lead to good solutions, and reduce the the influence of the function that lead to not so good solutions. The reinforcement can be achieved by adding a new PDF function, and the decrease can be achieved by removing PDFs from the kernel mixture.				
						For more details in the algorithm see \cite{socha04,socha08}.
					
	\pagebreak
			
	\section{Cluster Geometry Optimization}
		\label{subsec:cgo}
		Here we present some algorithms that were proposed in the literature to tackle the problem of Cluster Geometry Optimization, using the Morse potential as a benchmark function.

		One of the first attempts of global optimization of the Morse potential was made by Wales et al. \cite{wales97, doye97}. In their work they used a Monte Carlo minimization method \cite{li87} combined with a local search algorithm. This method is known as Basin-Hopping (BH). In the work conducted by Wales et al., they deform the surface of the PES, in a way that some of the valleys are removed. This transformation does not affect the energies of the minima. They were able to found almost all the known global optima for short-ranged ($\beta = 14.0$) Morse clusters up to 80 atoms.
		
		Roberts et al. \cite{roberts00} proposed the first EA to the optimization of Morse clusters. They used a certain number of components that had been previously proposed: real-valued representation of the Cartesian coordinates of the atoms, a local search method to improve the solutions, and the Cut and Splice \cite{deaven95} crossover. This crossover is a specific operator handling atomic clusters. First, it defines a plane that passes through the center of mass of the clusters to be combined. Then the clusters are cut by this plane and the complementary halves are joined (spliced) together in order to form a new offspring.
		The mutation operator in this work corresponds to assign random coordinates to a certain number of atoms. 
		The algorithm was applied to instances between 19 and 50 of the Morse Potential with $\beta = 6.0$ and $\beta = 14.0$. The results showed that it was able to discover nearly all the known global optima \cite{roberts00}.   
		
		Grosso et al. \cite{grosso07} proposed Population Basin Hopping (PBH), a stochastic method, in which different conformations of the clusters are maintained in a population of solutions, hence ensuring a sufficient level of diversity. The diversity is enforced through the definition of problem specific diversity measures. Moreover, in their work, Grosso et el. applied a two-phase local search algorithm, in order to improve the efficiency of the method.
		
		Dynamic lattice searching (DLS) is another effective approach for cluster geometry optimization \cite{cheng07}. It starts with a randomly generated local minimum and iteratively applies a greedy strategy to search for better solutions in the neighborhood. This search is aid by a Dynamic Lattice (DL). The DL is constructed adaptively, based on a starting local minimum, and then locating all the possible locations for new atom. Afterwards, they perform a greedy search for conformations with low potential energy values: it iteratively moves the atom located at the position with the highest energy (in the lattice) to the vacant position with the lowest energy (in the lattice) \cite{cheng07, shao04}. The method is restarted several times to ensure proper exploration of the search space \cite{cheng07}.
			
		Pereira et al.\cite{xico09} proposed a steady-state EA, with diversity control to the problem of cluster geometry optimization. In a steady-state EA, after generating a new offspring it is necessary to decide if it will enter the population, and if it does, which individual it will replace. In general, replacement strategies are related with the fitness and/or the age of individuals. Fitness based strategies check the quality of the individuals to decide who will be replaced. In the age based strategies the oldest elements are replaced. These two strategies are not mutually exclusive, since they can be combined. Moreover, these strategies can still be combined with other mechanisms, in order to keep a suitable level of diversity in the populations.
		In Pereira et al.\cite{xico09} they propose a couple of mechanisms to keep the diversity in the population. They showed that the diversity is important to improve the effectiveness of the EA. The proposed EA was able to find all the known optima for short-ranged Morse clusters up to 80 atoms.
		
		More recently, Lourenço et al. \cite{lourenco11} proposed a Particle Swarm Optimization algorithm to the problem of cluster geometry optimization. The proposed algorithm has several features that were specifically design to tackle this problem. The most noteworthy are: the adoption of specific rules to update current position of particles, where the velocity is only applied to fraction of the solution; and the embracing of a steady-state strategy to update the population particles. This strategy allows for a simultaneous exploration of the neighborhoods of the current locations, and of the best ever seen solutions. Additionally it has a diversity mechanism, to postpone convergence.
 		The results presented showed that the algorithm was able to find all the best-known solutions to short-ranged Morse instances between 30 and 50 atoms.
		
