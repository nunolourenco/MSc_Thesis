%!TEX root = /Users/nunolourenco/Documents/FCTUC/Mestrado/2010_2011/Thesis/Thesis/thesis.tex
\chapter{Results}

In this chapter we present an experimental analysis of the application of our algorithm to several instances of short-ranged Morse Clusters. To perform such analysis, we describe the scenario used in section \ref{sec:experimental_scenario}. In the following sections we present the obtained results.




\section{Experimental Scenario}
\label{sec:experimental_scenario}
In this study we focus our attention in several instances of short-ranged Morse clusters. More precisely, we selected instances with a number of atoms that ranges between 30 and 50. With this scenario we aim for two goals: 
	\begin{enumerate}
		\item Assess the performance of the algorithm;
		\item Gain insight into the influence of some components of the algorithm. 
	\end{enumerate}
	
	For the first objective, we present the results of the CGACO optimization for all the mentioned instances, and we analyze its performance based on two criteria:
	\begin{enumerate}
		\item Ability to discover the known optima;
		\item Mean Best Fitness (MBF) deviation from the optimum;
	\end{enumerate}
	
	The first criterion is a very widely adopted performance measure in cluster geometry optimization. Hence, for all instances, we show the best solution found by the algorithm, and what was its success rate(number of times that it found the best-known solution).
	The second criterion aims to complement our study, as it will provide information about the convergence, of CGACO, to promising areas of the search space.
	
	Then, to assess the absolute performance of the CGACO, we compare its results with those achieved by two algorithms: a Genetic Algorithm described in Pereira et al. \cite{xico09}, and Particle Swarm Optimization (PSO) algorithm. The PSO algorithm is an swarm intelligence algorithm, and it combines a set of rules to improve its effectiveness. The most noteworthy are: a steady-state strategy to update solutions, with a structural distance measure, and new rule to update the particles, which applies the velocity only to a subset of the variables.
	
\section{Experimental Settings}
\label{sec:experimental_setting}

Table \ref{tab:general_settings} lists the general parameters used in all of the experiments. We performed a total of 30 runs to make possible a statistical analysis. It is important to refer that each iteration made my L-BFGS procedure count as one evaluation. This parameters were set based both on the reviewed literature, and some preliminary tests. The size of the population depends on the size of the cluster being optimized. 
The cell size corresponds to size of each cell in our search space. The neighborhood used is the Moore neighborhood with $r = 1$. The number of iterations in the discrete local search is 10.

\begin{table}[!htbp]
	\begin{center}
		\begin{tabular}{| l | p{8cm} |}
			\hline
			\textbf{Parameter} & \textbf{Value} \\ \hline
			Runs & 30 \\
			Population size & Equal to the number of atoms $N$ of the cluster \\
			Number of evaluations & 5000000 \\
			Morse Potential range $(\beta)$ & 14.0 \\ 
			Cell size $(W)$ & 0.6 \\
			Number of Atoms $(N)$ & Between 30 and 50 \\
			Influence of the pheromones $(\alpha)$) & 4 \\
			Continuous Local Search Iterations & 1000\\
			Continuous Local Search Accuracy & 1.0E-8\\
			Pheromone propagation value ($p$) & 0.5 \\
			Neighborhood type & Moore Neighborhood \\
			Neighborhood radius $(r)$ & 1 \\
			Discrete Local Search Iteration & 10 \\
			\hline
		\end{tabular}
	\caption{Parameter setting used in the experiments}
	\label{tab:general_settings}
	\end{center}
\end{table}


\section{Statistical Analysis}
To compare our algorithm with the EA and the PSO we performed a statistical analysis to validate the results. We assumed that our data didn't not followed any distribution. Thus we applied the Mann-Whitney non-parametric test, at a 0.05 level of significance, to assess the statistical differences of the means, over the 30 runs of each of pair CGACO-EA and CGACO-PSO of algorithms.

The hypothesis for comparing the two algorithms were:
\begin{itemize}
	\item $H_{0} : u_1 = u_2$ - means of the algorithms were equal, as the null hypothesis.
	\item $H_{1} : u_1 \neq u_2$ - means of the algorithms were not equal, as the alternative hypothesis.
\end{itemize}

Each time we applied a statistical test we looked for the results and concluded: if the $p$-value of the statistical test was smaller than the level of confidence, there was evidence to reject the null hypothesis $H_{0}$, and we could accept the alternative $H_{1}$. This means that there was evidence that the means were significantly different  at the significance level of 0.05. In contrast, there was not enough evidence to reject $H_{0}$, and being so, we concluded that means were not significantly different.



\section{Experimental Results}
	\subsection{CGACO}
	In Table  we present the results obtained by the CGACO algorithm, for the instances where it was applied. 
	\begin{table}[!htbp]
		\begin{center}
			\begin{tabular}{| c | p{2cm} | p{3cm} | p{2cm} | c | c |}
				\hline
				\textbf{Instance} & \textbf{Known Optimum} & \textbf{Best Solution Found} & \textbf{Success Rate} & \textbf{MBF} & \textbf{Deviation}\\
				\hline
			\end{tabular}
		\caption{Parameter setting used in the experiments}
		\label{tab:general_settings}
		\end{center}
	\end{table}
	
	
	
	
	
	
	
	
	
	

	